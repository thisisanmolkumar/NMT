{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh1uV7OaGXwt"
      },
      "source": [
        "# Neural Machine Translator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQV8oOQN6rsc",
        "outputId": "49324003-9b02-4916-b7ae-96f4abcbfec9"
      },
      "source": [
        "%cd /content/drive/MyDrive/Colab/NMT\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab/NMT\n",
            "NMT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2bzs0L1DJ8t",
        "outputId": "02035825-7541-4093-d88c-60265ea571fe"
      },
      "source": [
        "!mkdir Data\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data  NMT\n",
            "/content/drive/MyDrive/Colab/NMT/Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkADPZ6rTEhw"
      },
      "source": [
        "%cd /content/drive/MyDrive/Colab/NMT/Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JMbrqk-GjIb"
      },
      "source": [
        "## Loading Dataset and Converting to model input format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znH3uZH4GlVG"
      },
      "source": [
        "### Downloading Wikipedia English-Spanish Dataset from OPUS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdkhTznc8yD7",
        "outputId": "84f34d6a-c49d-4097-c735-e577583e41d8"
      },
      "source": [
        "!wget -O enes.zip http://opus.nlpl.eu/download.php?f=Wikipedia/v1.0/moses/en-es.txt.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-21 06:11:01--  http://opus.nlpl.eu/download.php?f=Wikipedia/v1.0/moses/en-es.txt.zip\n",
            "Resolving opus.nlpl.eu (opus.nlpl.eu)... 193.166.25.9\n",
            "Connecting to opus.nlpl.eu (opus.nlpl.eu)|193.166.25.9|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://opus.nlpl.eu/download.php?f=Wikipedia/v1.0/moses/en-es.txt.zip [following]\n",
            "--2021-04-21 06:11:02--  https://opus.nlpl.eu/download.php?f=Wikipedia/v1.0/moses/en-es.txt.zip\n",
            "Connecting to opus.nlpl.eu (opus.nlpl.eu)|193.166.25.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://object.pouta.csc.fi/OPUS-Wikipedia/v1.0/moses/en-es.txt.zip [following]\n",
            "--2021-04-21 06:11:03--  https://object.pouta.csc.fi/OPUS-Wikipedia/v1.0/moses/en-es.txt.zip\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 189445871 (181M) [application/zip]\n",
            "Saving to: ‘enes.zip’\n",
            "\n",
            "enes.zip            100%[===================>] 180.67M  22.1MB/s    in 9.5s    \n",
            "\n",
            "2021-04-21 06:11:13 (19.0 MB/s) - ‘enes.zip’ saved [189445871/189445871]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-crIpgbyGnez"
      },
      "source": [
        "### Unzipping and checking the shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCdiBRdg9FT5",
        "outputId": "54041f3c-25a9-4a2b-c605-494cb6c90245"
      },
      "source": [
        "!unzip enes.zip\n",
        "!wc -lw Wikipedia.en-es.en Wikipedia.en-es.es"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  enes.zip\n",
            "  inflating: Wikipedia.en-es.en      \n",
            "  inflating: Wikipedia.en-es.es      \n",
            "  inflating: Wikipedia.en-es.ids     \n",
            "  inflating: README                  \n",
            "  1811428  35383771 Wikipedia.en-es.en\n",
            "  1811428  36136487 Wikipedia.en-es.es\n",
            "  3622856  71520258 total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqRsZz2aAfGg"
      },
      "source": [
        "with open('Wikipedia.en-es.ids', 'r') as id:\n",
        "    id = id.read().split('\\n')\n",
        "\n",
        "l = len(id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK8a3n0H9Lvb"
      },
      "source": [
        "with open('Wikipedia.en-es.en', 'r') as enFile, open('Wikipedia.en-es.es', 'r') as esFile:\n",
        "    en = enFile.read().split('\\n')\n",
        "    es = esFile.read().split('\\n')\n",
        "\n",
        "data = ''\n",
        "for i in range(l):\n",
        "    data += en[i] + '\\t' + es[i] + '\\n'\n",
        "\n",
        "with open('Data-en-es.txt', 'w') as dataFile:\n",
        "    dataFile.write(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIQ0z0O42_TB"
      },
      "source": [
        "## Importing Libraries and Installing Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zziXikjFGg6"
      },
      "source": [
        "import torch\n",
        "import torch.cuda\n",
        "import unicodedata\n",
        "import string\n",
        "from random import shuffle\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTiPdwVLGfzG"
      },
      "source": [
        "### Checking for GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yiq9DGZEh03",
        "outputId": "cbfa0367-10e5-4fdc-b19d-9ade76e6c821"
      },
      "source": [
        "GPU = torch.cuda.is_available()\n",
        "print(GPU)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdaqjuT4Gx2G"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p2pUajku1Kq"
      },
      "source": [
        "### Cleaning the data\n",
        "*   Unicode string to plain ASCII\n",
        "*   Converting into lower case and removing punctuation\n",
        "*   Removing sentences which have word length greater than max length given"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id25hO99FMtf"
      },
      "source": [
        "def cleanData(s):\n",
        "    sent = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')  # Mn stands for Nonspacing_Mark\n",
        "    return sent.lower().translate(sent.maketrans('', '', string.punctuation))\n",
        "\n",
        "def filterPairs(pairs, maxLength):\n",
        "    return [p for p in pairs if len(p[0].split()) < maxLength and len(p[1].split()) < maxLength]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvSLlWjHvrR7"
      },
      "source": [
        "### Creating One Hot Encoded vectors for the input and output languages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwJXIO_Uv81y"
      },
      "source": [
        "class Lang:\n",
        "    def __init__(self, language):\n",
        "        self.langName = language\n",
        "        self.wordToIndex = {'SOS':0, 'EOS':1, '<UNK>':2}\n",
        "        self.wordToCount = {}\n",
        "        self.indexToWord = {0:'SOS', 1:'EOS', 2:'<UNK>'}\n",
        "        self.vocabSize = 3\n",
        "        self.cutoff = -1\n",
        "\n",
        "    # Counts the occurence of each token and saves to wordToCount\n",
        "    def countTokens(self, sent):\n",
        "        for word in sent.split():\n",
        "            if word not in self.wordToCount:\n",
        "                self.wordToCount[word] = 1\n",
        "            else:\n",
        "                self.wordToCount[word] += 1\n",
        "\n",
        "    # Adds each unique token into wordToIndex and indexToWord attributes with unique index\n",
        "    # Replacing words with less frequency with <UNK> \n",
        "    def addTokens(self, sent):\n",
        "        newSent = ''\n",
        "        for word in sent.split():\n",
        "            newWord = ''\n",
        "            if self.wordToCount[word] > self.cutoff:\n",
        "                if word not in self.wordToIndex:\n",
        "                    self.wordToIndex[word] = self.vocabSize\n",
        "                    self.indexToWord[self.vocabSize] = word\n",
        "                    self.vocabSize += 1\n",
        "                newWord = word\n",
        "            else:\n",
        "                newWord = self.indexToWord[2]\n",
        "\n",
        "            newSent += newWord + ' ' \n",
        "\n",
        "        return newSent\n",
        "\n",
        "    # Creates a cutoff to leave infrequent words\n",
        "    def createCutoff(self, vocSizeMax):\n",
        "        frequency = list(self.wordToCount.values()).sort(reverse=True)\n",
        "\n",
        "        if len(frequency) > vocSizeMax:\n",
        "            self.cutoff = frequency[vocSizeMax]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM-2tL06xU88"
      },
      "source": [
        "### Creating Input and Output Language datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ylp63yALEElI"
      },
      "source": [
        "# def langPrepare(lang1, lang2, path, rev=False):\n",
        "#     sentToken = open(path, encoding='utf-8').read().split('\\n')\n",
        "#     pairs = [[cleanData(s) for s in t.split('\\t')] for t in sentToken]\n",
        "\n",
        "#     if reverse:\n",
        "#         pairs = [list(reversed(p)) for p in pairs]\n",
        "#         inpLang = Lang(lang2)\n",
        "#         outLang = Lang(lang1)\n",
        "#     else:\n",
        "#         inpLang = Lang(lang1)\n",
        "#         outLang = Lang(lang2)\n",
        "\n",
        "#     return inpLang, outLang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ-vbB9UGT0G"
      },
      "source": [
        "def dataPrepare(lang1, lang2, path, vocSizeMax=50000, rev=False, maxLength=0, div=0.9):\n",
        "    # inpLang, outLang, pairs = langPrepare(lang1, lang2, path, rev)\n",
        "    sentToken = open(path, encoding='utf-8').read().split('\\n')\n",
        "    pairs = [[cleanData(s) for s in t.split('\\t')] for t in sentToken]\n",
        "\n",
        "    if rev:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        inpLang = Lang(lang2)\n",
        "        outLang = Lang(lang1)\n",
        "    else:\n",
        "        inpLang = Lang(lang1)\n",
        "        outLang = Lang(lang2)\n",
        "\n",
        "    if maxLength != 0:\n",
        "        pairs = filterPairs(pairs, maxLength)\n",
        "\n",
        "    for pair in pairs:\n",
        "        inpLang.countTokens(pair[0])\n",
        "        outLang.countTokens(pair[1])\n",
        "\n",
        "    inpLang.createCutoff(vocSizeMax)\n",
        "    outLang.createCutoff(vocSizeMax)\n",
        "\n",
        "    pairs = [(inpLang.addTokens(pair[0]), outLang.addTokens(pair[1])) for pair in pairs]\n",
        "\n",
        "    shuffle(pairs)\n",
        "\n",
        "    trainData = pairs[:math.ceil(div * len(pairs))]\n",
        "    testData = pairs[math.ceil(div * len(pairs)):]\n",
        "\n",
        "    print(f\"Train: {len(trainData)} Test: {len(testData)}\")\n",
        "    print(\"Counted Words -> Trimmed Vocabulary Sizes (w/ EOS and SOS tags):\")\n",
        "    print(\"%s, %s -> %s\" % (inpLang.langName, len(inpLang.wordToCount),\n",
        "                            inpLang.vocabSize))\n",
        "    print(\"%s, %s -> %s\" % (outLang.langName, len(outLang.wordToCount), \n",
        "                            outLang.vocabSize))\n",
        "    \n",
        "    return inpLang, outLang, trainData, testData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzMxbFKb-B9e"
      },
      "source": [
        "### Converting to and from One Hot Encoded vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfJzrGnN7Keb"
      },
      "source": [
        "def sentToTensor(lang, sent):\n",
        "    ind = [lang.wordToIndex[word] if word in lang.wordToIndex else lang.wordToIndex[\"<UNK>\"] for word in sent.split()]\n",
        "    ind.append(1)  # EOS Token\n",
        "    res = torch.LongTensor(ind).view(-1)  # Converting into long tensor and reshaping the tensor\n",
        "\n",
        "    if GPU:\n",
        "        return res.cuda()\n",
        "    else:\n",
        "        return res\n",
        "\n",
        "def tensorToSent(lang, tens):\n",
        "    return ' '.join([lang.indexToWord[i.item()] for i in tensor.data])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96hZpKJN8pq6"
      },
      "source": [
        "def pairToTensor(lang1, lang2, pair):\n",
        "    return (sentToTensor(lang1, pair[0]), sentToTensor(lang2, pair[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGgbB26CP9ih"
      },
      "source": [
        "### Creating batches to train using Mini Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2OJpQpGQHqa"
      },
      "source": [
        "def miniBatch(pairs, lang1, lang2, size):\n",
        "    shuffle(data)\n",
        "\n",
        "    nBatches = len(pairs) // size\n",
        "\n",
        "    batchList = longElemList = [0 for _ in range(nBatches)]\n",
        "    \n",
        "    for i in range(nBatches):\n",
        "        lInp = lOut = 0\n",
        "        vInp = vOut = [0 for _ in range(size)]\n",
        "        \n",
        "        for j in range((i * size), ((i + 1) * size)):\n",
        "            vInp[j], vOut[j] = tensorsFromPair(lang1, lang2, pairs[j])\n",
        "\n",
        "            if len(vInp[j]) >= lInp:\n",
        "                lInp = len(vInp[j])\n",
        "            if len(vOut[j]) >= lOut:\n",
        "                lOut = len(vOut[j])\n",
        "        \n",
        "        batchList[i] = (vInp, vOut)\n",
        "        longElemList[i] = (lInp, lOut)\n",
        "\n",
        "    return batchList, longElemList, nBatches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJil2vGLSWsb"
      },
      "source": [
        "### Padding with <EOS> Token to make every batch of equal length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbmtOm2VSelB"
      },
      "source": [
        "def padding(batch):\n",
        "    inp = torch.nn.utils.rnn.pad_sequence(batch[0], padding_value=1)\n",
        "    out = torch.nn.utils.rnn.pad_sequence(batch[1], padding_value=1)\n",
        "    return (inp, out)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}